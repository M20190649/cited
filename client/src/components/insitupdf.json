{
"paper": {
    "title": "In Situ Generated Probability Distribution Functions for Interactive Post Hoc Visualization and Analysis",
    "jornal": "LDAV",
    "year": 2016,
    "citationCount": 1,
    "authors": "Yucong (Chris) Ye, Tyson Neuroth, Franz Sauer, Kwan-Liu Ma, Giulio Borghesi, Aditya Konduri, Hemanth Kolla, and Jacqueline Chen"
},
"references": [
    {
        "title": "Scientific discovery at the exascale. report from the DOE ASCR 2011",
        "abstract": "In February 2011, the Department of Energy (DOE) Office of Advanced Scientific Computing Research (ASCR) convened a workshop to explore the problem of scientific understanding of data from High Performance Computation (HPC) at the exascale. The goal of this workshop report is to identify the research and production directions that the Data Management, Analysis, and Visualization (DMAV) community must take to enable scientific discovery for HPC as it approaches the exascale (1 quintillion floating point calculations per second = 1 exaflop). Projections from the international TOP500 list [20] place that date around 2018–2019. Extracting scientific insight from large HPC facilities is of crucial importance for the nation. The scientific simulations that run on the supercomputers are only half of the “science”; scientific advances are made only once the data produced by the simulations is processed into an output that is understandable by an application scientist. As mathematician Richard Hamming famously said, “The purpose of computing is insight, not numbers.” [29] It is precisely the DMAV community that provides the algorithms, research, and tools to enable that critical insight. The hardware and software changes that will occur as HPC enters the exascale era will be dramatic and disruptive. Not only are scientific simulations forecasted to grow by many orders of magnitude, but also current methods by which HPC systems are programmed and data are extracted are not expected to survive into the exascale. Changing the fundamental methods by which scientific understanding is obtained from HPC simulations is a daunting task. Specifically, dramatic changes to on-node concurrency, access to memory hierarchies, accelerator and GPGPU processing, and input/output (I/O) subsystems will all necessitate reformulating existing DMAV algorithms and workflows. Additionally, reductions in inter-node and off-machine communication bandwidth will require rethinking how best to provide scalable algorithms for scientific understanding. Principal Finding: The disruptive changes imposed by a progressive movement toward the exascale in HPC threaten to derail the scientific discovery process. Today’s successes in extracting knowledge from large HPC simulation output are not generally applicable to the exascale era, and simply scaling existing techniques to higher concurrency is insufficient to meet the challenge. Recommendation: Focused and concerted efforts toward co-designing processes for exascale scientific understanding must be adopted by DOE ASCR. These efforts must include research into effective in situ processing frameworks, new I/O middleware systems, fine-grained visualization and analysis algorithms to exploit future architectures, and co-scheduling analysis and simulation on HPC platforms. Such efforts must be pursued in direct collaboration with the application domain scientists targeting exascale architectures. To ensure effective delivery to scientists, DMAV researchers require access to “testbed” systems so that they can prototype and pilot effective solutions. Our full set of findings and recommendations may be found in Section 8 “Conclusion: Findings and Recommendations” on page 31.",
        "journal": "Workshop on Exascale Data Management, Analysis, and Visualization",
        "year": 2011,
        "citationCount": 25,
        "authors": "Sean Ahern, Arie Shoshani, Kwan-Liu Ma, Alok Choudhary, Terence Critchlow, Scott Klasky, Valerio Pascucci, Jim Ahrens, E. Wes Bethel, Hank Childs, Jian Huang, Ken Joy, Quincy Koziol, Gerald Lofstead, Jeremy Meredith, Kenneth Moreland, George Ostrouchov, Michael Papka, Venkatram Vishwanath, Matthew Wolf, Nicholas Wright, Kesheng Wu"
    }, {
        "title": "An image-based approach to extreme scale in situ visualization and analysis",
        "abstract": "Extreme scale scientific simulations are leading a charge to exascale computation, and data analytics runs the risk of being a bottleneck to scientific discovery. Due to power and I/O constraints, we expect in situ visualization and analysis will be a critical component of these workflows. Options for extreme scale data analysis are often presented as a stark contrast: write large files to disk for interactive, exploratory analysis, or perform in situ analysis to save detailed data about phenomena that a scientists knows about in advance. We present a novel framework for a third option - a highly interactive, image-based approach that promotes exploration of simulation results, and is easily accessed through extensions to widely used open source tools. This in situ approach supports interactive exploration of a wide range of results, while still significantly reducing data movement and storage.",
        "journal": "SC",
        "year": 2014,
        "citationCount": 80,
        "authors": "James Ahrens, Sébastien Jourdain, Patrick O'Leary, John Patchett, David H. Rogers, Mark Petersen"
    }, {
        "title": "Paraview catalyst: Enabling in situ data analysis and visualization",
        "abstract": "Computer simulations are growing in sophistication and producing results of ever greater fidelity. This trend has been enabled by advances in numerical methods and increasing computing power. Yet these advances come with several costs including massive increases in data size, difficulties examining output data, challenges in configuring simulation runs, and difficulty debugging running codes. Interactive visualization tools, like ParaView, have been used for post-processing of simulation results. However, the increasing data sizes, and limited storage and bandwidth make high fidelity post-processing impractical. In situ analysis is recognized as one of the ways to address these challenges. In situ analysis moves some of the post-processing tasks in line with the simulation code thus short circuiting the need to communicate the data between the simulation and analysis via storage. ParaView Catalyst is a data processing and visualization library that enables in situ analysis and visualization. Built on and designed to interoperate with the standard visualization toolkit VTK and the ParaView application, Catalyst enables simulations to intelligently perform analysis, generate relevant output data, and visualize results concurrent with a running simulation. In this paper, we provide an overview of the Catalyst framework and some of the success stories.",
        "journal": "ISAV",
        "year": 2015,
        "citationCount": 29,
        "authors": "Utkarsh Ayachit, Andrew Bauer, Berk Geveci, Patrick O'Leary, Kenneth Moreland, Nathan Fabian, Jeffrey Mauldin"
    }, {
        "title": "Contour tree depth images for large data visualization",
        "abstract": "High-fidelity simulation models on large-scale parallel computer systems can produce data at high computational throughput, but modern architectural trade-offs make full persistent storage to the slow I/O subsystem prohibitively costly with respect to time. We demonstrate the feasibility and potential of combining in situ topological contour tree analysis and compact image-based data representation to address this problem. Our experiments show significant reductions in storage requirements using topology-guided layered depth imaging, while preserving flexibility for explorative visualization and analysis. Our approach represents an effective and easy-to-control trade-off between storage overhead and visualization fidelity for large data visualization.",
        "journal": "EGPGV",
        "year": 2015,
        "citationCount": 6,
        "authors": "Tim Biedert, Christoph Garth"
    }, {
        "title": "Terascale direct numerical simulations of turbulent combustion using S3D",
        "abstract": "Computational science is paramount to the understanding of underlying processes in internal combustion engines of the future that will utilize non-petroleum-based alternative fuels, including carbon-neutral biofuels, and burn in new combustion regimes that will attain high efficiency while minimizing emissions of particulates and nitrogen oxides. Next-generation engines will likely operate at higher pressures, with greater amounts of dilution and utilize alternative fuels that exhibit a wide range of chemical and physical properties. Therefore, there is a significant role for high-fidelity simulations, direct numerical simulations (DNS), specifically designed to capture key turbulence-chemistry interactions in these relatively uncharted combustion regimes, and in particular, that can discriminate the effects of differences in fuel properties. In DNS, all of the relevant turbulence and flame scales are resolved numerically using high-order accurate numerical algorithms. As a consequence terascale DNS are computationally intensive, require massive amounts of computing power and generate tens of terabytes of data. Recent results from terascale DNS of turbulent flames are presented here, illustrating its role in elucidating flame stabilization mechanisms in a lifted turbulent hydrogen/air jet flame in a hot air coflow, and the flame structure of a fuel-lean turbulent premixed jet flame. Computing at this scale requires close collaborations between computer and combustion scientists to provide optimized scaleable algorithms and software for terascale simulations, efficient collective parallel I/O, tools for volume visualization of multiscale, multivariate data and automating the combustion workflow. The enabling computer science, applied to combustion science, is also required in many other terascale physics and engineering simulations. In particular, performance monitoring is used to identify the performance of key kernels in the DNS code, S3D and especially memory intensive loops in the code. Through the careful application of loop transformations, data reuse in cache is exploited thereby reducing memory bandwidth needs, and hence, improving S3D's nodal performance. To enhance collective parallel I/O in S3D, an MPI-I/O caching design is used to construct a two-stage write-behind method for improving the performance of write-only operations. The simulations generate tens of terabytes of data requiring analysis. Interactive exploration of the simulation data is enabled by multivariate time-varying volume visualization. The visualization highlights spatial and temporal correlations between multiple reactive scalar fields using an intuitive user interface based on parallel coordinates and time histogram. Finally, an automated combustion workflow is designed using Kepler to manage large-scale data movement, data morphing, and archival and to provide a graphical display of run-time diagnostics.",
        "journal": "Computational Science & Discovery",
        "year": 2009,
        "citationCount": 427,
        "authors": "Jacqueline Chen, A. Choudhary, B. De Supinski, M. DeVries, Evatt Hawkes, Scott Klasky, W. Liao, Kun Ling Ma, J. Mellor-Crummey, Norbert Podhorszki, Ramanan Sankaran, Sameer Shende, Chun Sang Yoo"
    }, {
        "title": "In situ processing",
        "abstract": "Traditionally, visualization is done via post-processing: a simulation produces data, writes that data to disk, and then, later, a separate visualization program reads the data from disk and operates on it. In situ processing refers to a different approach: the data is processed while it is being produced by the simulation, allowing visualization to occur without involving disk storage. As recent supercomputing trends have simulations producing data at a much faster rate than I/O bandwidth, in situ processing will likely play a bigger and bigger role in visualizing data sets on the world’s largest machines. The push towards commonplace in situ processing has a benefit besides saving on I/O costs. Already, scientists must limit how much data they store for later processing, potentially limiting their discoveries and the value of their simulations. In situ processing, however, enables the processing of this unexplored data.",
        "journal": "High Performance Visualization: Enabling Extreme-Scale Scientific Insight",
        "year": 2012,
        "citationCount": 19,
        "authors": "Hank Childs, Kwan-Liu Ma, Hongfeng Yu, Brad Whitlock, Jeremy Meredith, Jean Favre, Scott Klasky, Norbert Podhorszki, Karsten Schwan, Matthew Wolf, Manish Parashar, Fan Zhang"
    }, {
        "title": "Damaris/viz: a nonintrusive, adaptable and user-friendly in situ visualization framework",
        "abstract": "Reducing the amount of data stored by simulations will be of utmost importance for the next generation of large-scale computing. Accordingly, there is active research to shift analysis and visualization tasks to run in situ, that is, closer to the simulation via the sharing of some resources. This is beneficial as it can avoid the necessity of storing large amounts of data for post-processing. In this paper, we focus on the specific case of in situ visualization where analysis codes are collocated with the simulation's code and run on the same resources. It is important for an in situ technique to require minimum modifications to existing codes, be adaptable, and have a low impact on both run times and resource usage. We accomplish this through the Damaris/Viz framework, which provides in situ visualization support to the Damaris I/O middleware. The use of Damaris as a bridge to existing visualization packages allows us to (1) reduce code moditication to a minimum for existing simulations, (2) gather capabilities of several visualization tools to offer a unified data management interface, (3) use dedicated cores to hide the run time impact of in situ visualization and (4) efficiently use memory through a shared-memory-based communication model. Experiments are conducted on Blue Waters and Grid'5000 to visualize the CM1 atmospheric simulation and the Nek5000 CFD solver.",
        "journal": "LDAV",
        "year": 2013,
        "citationCount": 33,
        "authors": "Matthieu Dorier, Robert Sisneros, Tom Peterka, Gabriel Antoniu, Dave Semeraro"
    }, {
        "title": "Space-time volumetric depth images for in-situ visualization",
        "abstract": "Volumetric depth images (VDI) are a view-dependent representation that combines the high quality of images with the explorability of 3D fields. By compressing the scalar data along view rays into sets of coherent supersegments, VDIs provide an efficient representation that supports a-posteriori changes of camera parameters. In this paper, we introduce space-time VDIs that achieve the data reduction that is required for efficient in-situ visualization, while still maintaining spatiotemporal flexibility. We provide an efficient space-time representation of VDI streams by exploiting inter-ray and inter-frame coherence, and introduce delta encoding for further data reduction. Our space-time VDI approach exhibits small computational overhead, is easy to integrate into existing simulation environments, and may help pave the way toward exascale computing.",
        "journal": "LDAV",
        "year": 2014,
        "citationCount": 10,
        "authors": "Oliver Fernandes, Steffen Frey, Filip Sadlo, Thomas Ertl"
    }, {
        "title": "An approach to exascale visualization: Interactive viewing of in-situ visualization",
        "abstract": "In the coming era of exascale supercomputing, in-situ visualization will be a crucial approach for reducing the output data size. A problem of in-situ visualization is that it loses interactivity if a steering method is not adopted. In this paper, we propose a new method for the interactive analysis of in-situ visualization images produced by a batch simulation job. A key idea is to apply numerous (thousands to millions) in-situ visualizations simultaneously. The viewer then analyzes the image database interactively during postprocessing. If each movie can be compressed to 100 MB, one million movies will only require 100 TB, which is smaller than the size of the raw numerical data in exascale supercomputing. We performed a feasibility study using the proposed method. Multiple movie files were produced by a simulation and they were analyzed using a specially designed movie player. The user could change the viewing angle, the visualization method, and the parameters interactively by retrieving an appropriate sequence of images from the movie dataset.",
        "journal": "CPC",
        "year": 2014,
        "citationCount": 29,
        "authors": "Akira Kageyama, Tomoki Yamada"
    }, {
        "title": "Parallel in situ indexing for data-intensive computing",
        "abstract": "As computing power increases exponentially, vast amount of data is created by many scientific research activities. However, the bandwidth for storing the data to disks and reading the data from disks has been improving at a much slower pace. These two trends produce an ever-widening data access gap. Our work brings together two distinct technologies to address this data access issue: indexing and in situ processing. From decades of database research literature, we know that indexing is an effective way to address the data access issue, particularly for accessing relatively small fraction of data records. As data sets increase in sizes, more and more analysts need to use selective data access, which makes indexing an even more important for improving data access. The challenge is that most implementations of indexing technology are embedded in large database management systems (DBMS), but most scientific datasets are not managed by any DBMS. In this work, we choose to include indexes with the scientific data instead of requiring the data to be loaded into a DBMS.We use compressed bitmap indexes from the FastBit software which are known to be highly effective for query-intensive workloads common to scientific data analysis. To use the indexes, we need to build them first. The index building procedure needs to access the whole data set and may also require a significant amount of compute time. In this work, we adapt the in situ processing technology to generate the indexes, thus removing the need of reading data from disks and to build indexes in parallel. The in situ data processing system used is ADIOS, a middleware for high-performance I/O. Our experimental results show that the indexes can improve the data access time up to 200 times depending on the fraction of data selected, and using in situ data processing system can effectively reduce the time needed to create the indexes, up to 10 times with our in situ technique when using identical parallel settings.",
        "journal": "LDAV",
        "year": 2011,
        "citationCount": 38,
        "authors": "Jinoh Kim, Hasan Abbasi, Luis Chacón, Ciprian Docan, Scott Klasky, Qing Liu, Norbert Podhorszki, Arie Shoshani, Kesheng Wu"
    }, {
        "title": "In situ data processing for extreme scale computing",
        "abstract": "The DOE leadership facilities were established in 2004 to provide scientist capability computing for high-profile science. Since it’s inception, the systems went from 14 TF to 1.8 PF, an increase of 100 in 5 years, and will increase by another factor of 100 in 5 more years. This growth, along with user policies, which enable scientist to run at, scale for long periods of time, have allowed scientist to write unprecedented amounts of data to the file system. In the same time, the effective speed of the I/O system (time to write full system memory to the file system) has decreased, going from 49 TB/140 GB/s on ASCI purple, to 300 TB/200 GB/s on jaguar. As future systems will further intensify this imbalance, we need to extend the definition of I/O to include that of I/O pipelines, which blend together; 1) Self describing file formats, 2) Staging methods with visualization and analysis “plug-ins”, 3) new techniques to multiplex outputs using a novel Service Oriented Approach (SOA), in a 4) Easy-to-use I/O componentization framework which can add resiliency to large scale calculations. Our approach to this has been in the community created ADIOS system, driven by many of the leading edge application teams, and by many of the top I/O researchers.",
        "journal": "SciDAC",
        "year": 2011,
        "citationCount": 33,
        "authors": "Scott Klasky, Hasan Abbasi, Jeremy Logan, Manish Parashar, Karsten Schwan, Arie Shoshani, Matthew Wolf, Sean Ahern, Ilkay Altintas, Wes Bethel, Luis Chacon, CS Chang, Jackie Chen, Hank Childs, Julian Cummings, Ciprian Docan, Greg Eisenhauer, Stephane Ethier, Ray Grout, Sriram Lakshminarasimhan, Zhihong Lin, Qing Liu, Xiaosong Ma, Kenneth Moreland, Valerio Pascucci, Norbert Podhorszki, Nagiza Samatova, Will Schroeder, Roselyne Tchoua, Yuan Tian, Raju Vatsavai, John Wu, Weikuan Yu, Fang Zheng"
    }, {
        "title": "Isabela for effective in situ compression of scientific data",
        "abstract": "Exploding dataset sizes from extreme-scale scientific simulations necessitates efficient data management and reduction schemes to mitigate I/O costs. With the discrepancy between I/O bandwidth and computational power, scientists are forced to capture data infrequently, thereby making data collection an inherently lossy process. Although data compression can be an effective solution, the random nature of real-valued scientific datasets renders lossless compression routines ineffective. These techniques also impose significant overhead during decompression, making them unsuitable for data analysis and visualization, which require repeated data access. To address this problem, we propose an effective method for In situ Sort-And-B-spline Error-bounded Lossy Abatement (ISABELA) of scientific data that is widely regarded as effectively incompressible. With ISABELA, we apply a pre-conditioner to seemingly random and noisy data along spatial resolution to achieve an accurate fitting model that guarantees a > 0.99 correlation with the original data. We further take advantage of temporal patterns in scientific data to compress data by 85%, while introducing only a negligible overhead on simulations in terms of runtime. ISABELA significantly outperforms existing lossy compression methods, such as wavelet compression, in terms of data reduction and accuracy. We extend upon our previous paper by additionally building a communication-free, scalable parallel storage framework on top of ISABELA-compressed data that is ideally suited for extreme-scale analytical processing. The basis for our storage framework is an inherently local decompression method (it need not decode the entire data), which allows for random access decompression and low-overhead task division that can be exploited over heterogeneous architectures. Furthermore, analytical operations such as correlation and query processing run quickly and accurately over data in the compressed space. Copyright © 2012 John Wiley & Sons, Ltd.",
        "journal": "CCPE",
        "year": 2013,
        "citationCount": 30,
        "authors": "Sriram Lakshminarasimhan, Neil Shah, Stephane Ethier, Seung‐Hoe Ku, C. S. Chang, Scott Klasky Rob Latham, Rob Ross, Nagiza F. Samatova"
    }, {
        "title": "In-situ feature extraction of large scale combustion simulations using segmented merge trees",
        "abstract": "The ever increasing amount of data generated by scientific simulations coupled with system I/O constraints are fueling a need for in-situ analysis techniques. Of particular interest are approaches that produce reduced data representations while maintaining the ability to redefine, extract, and study features in a post-process to obtain scientific insights. This paper presents two variants of in-situ feature extraction techniques using segmented merge trees, which encode a wide range of threshold based features. The first approach is a fast, low communication cost technique that generates an exact solution but has limited scalability. The second is a scalable, local approximation that nevertheless is guaranteed to correctly extract all features up to a predefined size. We demonstrate both variants using some of the largest combustion simulations available on leadership class supercomputers. Our approach allows state-of-the-art, feature-based analysis to be performed in-situ at significantly higher frequency than currently possible and with negligible impact on the overall simulation runtime.",
        "journal": "SC",
        "year": 2014,
        "citationCount": 38,
        "authors": "Aaditya G. Landge, Valerio Pascucci, Attila Gyulassy, Janine C. Bennett, Hemanth Kolla, Jacqueline Chen, Peer-Timo Bremer"
    }, {
        "title": "Adaptable, metadata rich io methods for portable high performance io",
        "abstract": "Since IO performance on HPC machines strongly depends on machine characteristics and configuration, it is important to carefully tune IO libraries and make good use of appropriate library APIs. For instance, on current petascale machines, independent IO tends to outperform collective IO, in part due to bottlenecks at the metadata server. The problem is exacerbated by scaling issues, since each IO library scales differently on each machine, and typically, operates efficiently to different levels of scaling on different machines. With scientific codes being run on a variety of HPC resources, efficient code execution requires us to address three important issues: (1) end users should be able to select the most efficient IO methods for their codes, with minimal effort in terms of code updates or alterations; (2) such performance-driven choices should not prevent data from being stored in the desired file formats, since those are crucial for later data analysis; and (3) it is important to have efficient ways of identifying and selecting certain data for analysis, to help end users cope with the flood of data produced by high end codes. This paper employs ADIOS, the adaptable IO system, as an IO API to address (1)-(3) above. Concerning (1), ADIOS makes it possible to independently select the IO methods being used by each grouping of data in an application, so that end users can use those IO methods that exhibit best performance based on both IO patterns and the underlying hardware. In this paper, we also use this facility of ADIOS to experimentally evaluate on petascale machines alternative methods for high performance IO. Specific examples studied include methods that use strong file consistency vs. delayed parallel data consistency, as that provided by MPI-IO or POSIX IO. Concerning (2), to avoid linking IO methods to specific file formats and attain high IO performance, ADIOS introduces an efficient intermediate file format, termed BP, which can be converted, at small cost, to the standard file formats used by analysis tools, such as NetCDF and HDF-5. Concerning (3), associated with BP are efficient methods for data characterization, which compute attributes that can be used to identify data sets without having to inspect or analyze the entire data contents of large files.",
        "journal": "IPDPS",
        "year": 2009,
        "citationCount": 161,
        "authors": "Jay Lofstead, Fang Zheng, Scott Klasky, Karsten Schwan"
    }, {
        "title": "In-situ processing and visualization for ultrascale simulations",
        "abstract": "The growing power of parallel supercomputers gives scientists the ability to simulate more complex problems at higher fidelity, leading to many high-impact scientific advances. To maximize the utilization of the vast amount of data generated by these simulations, scientists also need scalable solutions for studying their data to different extents and at different abstraction levels. As we move into peta- and exa-scale computing, simply dumping as much raw simulation data as the storage capacity allows for post-processing analysis and visualization is no longer a viable approach. A common practice is to use a separate parallel computer to prepare data for subsequent analysis and visualization. A naive realization of this strategy not only limits the amount of data that can be saved, but also turns I/O into a performance bottleneck when using a large parallel system. We conjecture that the most plausible solution for the peta- and exa-scale data problem is to reduce or transform the data in-situ as it is being generated, so the amount of data that must be transferred over the network is kept to a minimum. In this paper, we discuss different approaches to in-situ processing and visualization as well as the results of our preliminary study using large-scale simulation codes on massively parallel supercomputers.",
        "journal": "JPCS",
        "year": 2007,
        "citationCount": 83,
        "authors": "Kwan-Liu Ma, Chaoli Wang, Hongfeng Yu, Anna Tikhonova"
    }, {
        "title": "Scalable visualization of discrete velocity decompositions using spatially organized histograms",
        "abstract": "Visualizing the velocity decomposition of a group of objects has applications to many studied data types, such as Lagrangian-based flow data or geospatial movement data. Traditional visualization techniques are often subject to a trade-off between visual clutter and loss of detail, especially in a large scale setting. The use of 2D velocity histograms can alleviate these issues. While they have been used throughout domain specific areas on a basic level, there has been very little work in the visualization community on leveraging them to perform more advanced visualization tasks. In this work, we develop an interactive system which utilizes velocity histograms to visualize the velocity decomposition of a group of objects. In addition, we extend our tool to utilize two schemes for histogram generation: an on-the-fly sampling scheme as well as an in situ scheme to maintain interactivity in extreme scale applications.",
        "journal": "LDAV",
        "year": 2015,
        "citationCount": 3,
        "authors": "Tyson Neuroth, Franz Sauer, Weixing Wang, Stephane Ethier, Kwan-Liu Ma"
    }, {
        "title": "Outlier-preserving focus+context visualization in parallel coordinates",
        "abstract": "Focus+context visualization integrates a visually accentuated representation of selected data items in focus (more details, more opacity, etc.) with a visually deemphasized representation of the rest of the data, i.e., the context. The role of context visualization is to provide an overview of the data for improved user orientation and improved navigation. A good overview comprises the representation of both outliers and trends. Up to now, however, context visualization not really treated outliers sufficiently. In this paper we present a new approach to focus+context visualization in parallel coordinates which is truthful to outliers in the sense that small-scale features are detected before visualization and then treated specially during context visualization. Generally, we present a solution which enables context visualization at several levels of abstraction, both for the representation of outliers and trends. We introduce outlier detection and context generation to parallel coordinates on the basis of a binned data representation. This leads to an output-oriented visualization approach which means that only those parts of the visualization process are executed which actually affect the final rendering. Accordingly, the performance of this solution is much more dependent on the visualization size than on the data size which makes it especially interesting for large datasets. Previous approaches are outperformed, the new solution was successfully applied to datasets with up to 3 million data records and up to 50 dimensions",
        "journal": "TVCG",
        "year": 2006,
        "citationCount": 221,
        "authors": "Matej Novotny, Helwig Hauser"
    }, {
        "title": "Isobar hybrid compression-I/O interleaving for large-scale parallel I/O optimization",
        "abstract": "Current peta-scale data analytics frameworks suffer from a significant performance bottleneck due to an imbalance between their enormous computational power and limited I/O bandwidth. Using data compression schemes to reduce the amount of I/O activity is a promising approach to addressing this problem. In this paper, we propose a hybrid framework for interleaving I/O with data compression to achieve improved I/O throughput side-by-side with reduced dataset size. We evaluate several interleaving strategies, present theoretical models, and evaluate the efficiency and scalability of our approach through comparative analysis. With our theoretical model, considering 19 real-world scientific datasets both from the public domain and peta-scale simulations, we estimate that the hybrid method can result in a 12 to 46 increase in throughput on hard-to-compress scientific datasets. At the reported peak bandwidth of 60 GB/s of uncompressed data for a current, leadership-class parallel I/O system, this translates into an effective gain of 7 to 28 GB/s in aggregate throughput.",
        "journal": "HPDC",
        "year": 2012,
        "citationCount": 33,
        "authors": "Eric R. Schendel, Saurabh V. Pendse, John Jenkins, David A. Boyuka II, Zhenhuan Gong, Sriram Lakshminarasimhan, Qing Liu, Hemanth Kolla, Jacqueline Chen, Scott Klasky, Robert Ross, Nagiza F Samatova"
    }, {
        "title": "Multi-resolution bitmap indexes for scientific data",
        "abstract": "The unique characteristics of scientific data and queries cause traditional indexing techniques to perform poorly on scientific workloads, occupy excessive space, or both. Refinements of bitmap indexes have been proposed previously as a solution to this problem. In this article, we describe the difficulties we encountered in deploying bitmap indexes with scientific data and queries from two real-world domains. In particular, previously proposed methods of binning, encoding, and compressing bitmap vectors either were quite slow for processing the large-range query conditions our scientists used, or required excessive storage space. Nor could the indexes easily be built or used on parallel platforms. In this article, we show how to solve these problems through the use of multi-resolution, parallelizable bitmap indexes, which support a fine-grained trade-off between storage requirements and query performance. Our experiments with large data sets from two scientific domains show that multi-resolution, parallelizable bitmap indexes occupy an acceptable amount of storage while improving range query performance by roughly a factor of 10, compared to a single-resolution bitmap index of reasonable size.",
        "journal": "TODS",
        "year": 2007,
        "citationCount": 126,
        "authors": "Rishi Rakesh Sinha, Marianne Winslett"
    }, {
        "title": "In-situ bitmaps generation and efficient data analysis based on bitmaps",
        "abstract": "Neither the memory capacity, memory access speeds, nor disk bandwidths are increasing at the same rate as the computing power in current and upcoming parallel machines. This has led to considerable recent research on in-situ data analytics. However, many open questions remain on how to perform such analytics, especially in memory constrained systems. Building on our earlier work that demonstrated bitmap indices (bitmaps) can be a suitable summary structure for key (offline) analytics tasks, this paper develops an in-situ analysis approach that performs data reduction (such as time-steps selection) using just bitmaps, and subsequently, stores only the selected bitmaps for post-analysis. We construct compressed bitmaps on the fly, show that many kinds of in-situ analyses can be supported by bitmaps without requiring the original data (and thus reducing memory requirements for in-situ analysis), and instead of writing the original simulation output, we only write the selected bitmaps to the disks (reducing the I/O requirements). We also demonstrate that we are able to use bitmaps for key offline analysis steps. We extensively evaluate our method with different simulations and applications, and demonstrate the effectiveness of our approach.",
        "journal": "HPDC",
        "year": 2015,
        "citationCount": 27,
        "authors": "Yu Su, Yi Wang, Gagan Agrawal"
    }, {
        "title": "Analysis of large-scale scalar data using hixels",
        "abstract": "One of the greatest challenges for today's visualization and analysis communities is the massive amounts of data generated from state of the art simulations. Traditionally, the increase in spatial resolution has driven most of the data explosion, but more recently ensembles of simulations with multiple results per data point and stochastic simulations storing individual probability distributions are increasingly common. This paper introduces a new data representation for scalar data, called hixels, that stores a histogram of values for each sample point of a domain. The histograms may be created by spatial down-sampling, binning ensemble values, or polling values from a given distribution. In this manner, hixels form a compact yet information rich approximation of large scale data. In essence, hixels trade off data size and complexity for scalar-value “uncertainty”. Based on this new representation we propose new feature detection algorithms using a combination of topological and statistical methods. In particular, we show how to approximate topological structures from hixel data, extract structures from multi-modal distributions, and render uncertain isosurfaces. In all three cases we demonstrate how using hixels compares to traditional techniques and provide new capabilities to recover prominent features that would otherwise be either infeasible to compute or ambiguous to infer. We use a collection of computer tomography data and large scale combustion simulations to illustrate our techniques.",
        "journal": "LDAV",
        "year": 2011,
        "citationCount": 55,
        "authors": "David Thompson, Joshua A. Levine, Janine C. Bennett, Peer-Timo Bremer, Attila Gyulassy, Valerio Pascucci, Philippe P. Pébay"
    }, {
        "title": "A preview and exploratory technique for large-scale scientific simulations",
        "abstract": "Successful in-situ and remote visualization solutions must have minimal storage requirements and account for only a small percentage of supercomputing time. One solution that meets these requirements is to store a compact intermediate representation of the data, instead of a 3D volume itself. In addition to compression techniques, it is necessary to develop new intermediate data representations that exploit the manner in which samples are composited to generate an image. Recent work proposes the use of attenuation functions as a data representation that summarizes the distribution of attenuation along the rays. This representation goes beyond conventional static images and allows users to dynamically explore their data, for example, to change color and opacity parameters, without accessing the original 3D data. The computation and storage costs of this method may still be prohibitively expensive for large and time-varying data sets, thus limiting its applicability in real-world scenarios. In this paper, we present an efficient algorithm for computing attenuation functions in parallel. We exploit the fact that the distribution of attenuation can be constructed recursively from a hierarchy of blocks or intervals in the data, thus making the process highly parallelizeable. We have developed a library of routines that can be used in a distance visualization scenario or can be called directly from a simulation code to generate explorable images in-situ. Through a number of examples, we demonstrate the application of this work to large-scale scientific simulations in a real-world parallel environment with thousands of processors. We also explore various compression methods for reducing the size of the RAF and propose the use of kernel density estimation to compute an alternative RAF representation, which more closely represents the actual distribution of samples along a ray.",
        "journal": "EGPGV",
        "year": 2011,
        "citationCount": 26,
        "authors": "Anna Tikhonova, Hongfeng Yu, Carlos D. Correa, Jacqueline Chen, Kwan-Liu Ma"
    }, {
        "title": "Toward simulation-time data analysis and i/o acceleration on leadership-class systems",
        "abstract": "The performance mismatch between computing and I/O components of current-generation HPC systems has made I/O the critical bottleneck for scientific applications. It is therefore critical to make data movement as efficient as possible, and, to facilitate simulation-time data analysis and visualization to reduce the data written to storage. These will be of paramount importance to enabling us to glean novel insights from simulations. We present our work in GLEAN, a flexible framework for data-analysis and I/O acceleration at extreme scale. GLEAN leverages the data semantics of applications, and fully exploits the diverse system topologies and characteristics. We discuss the performance of GLEAN for simulation-time analysis and I/O acceleration with simulations at scale on leadership class systems.",
        "journal": "LDAV",
        "year": 2011,
        "citationCount": 87,
        "authors": "Venkatram Vishwanath, Mark Hereld, Michael E. Papka"
    }, {
        "title": "Parallel in situ coupling of simulation with a fully featured visualization system",
        "abstract": "There is a widening gap between compute performance and the ability to store computation results. Complex scientific codes are the most affected since they must save massive files containing meshes and fields for offline analysis. Time and storage costs instead dictate that data analysis and visualization be combined with the simulations themselves, being done in situ so data are transformed to a manageable size before they are stored. Earlier approaches to in situ processing involved combining specific visualization algorithms into the simulation code, limiting flexibility. We introduce a new library which instead allows a fully-featured visualization tool, VisIt, to request data as needed from the simulation and apply visualization algorithms in situ with minimal modification to the application code.",
        "journal": "EGPGV",
        "year": 2011,
        "citationCount": 140,
        "authors": "Brad Whitlock, Jean M. Favre, Jeremy S. Meredith"
    }, {
        "title": "In situ eddy analysis in a high-resolution ocean climate model",
        "abstract": "An eddy is a feature associated with a rotating body of fluid, surrounded by a ring of shearing fluid. In the ocean, eddies are 10 to 150 km in diameter, are spawned by boundary currents and baroclinic instabilities, may live for hundreds of days, and travel for hundreds of kilometers. Eddies are important in climate studies because they transport heat, salt, and nutrients through the world's oceans and are vessels of biological productivity. The study of eddies in global ocean-climate models requires large-scale, high-resolution simulations. This poses a problem for feasible (timely) eddy analysis, as ocean simulations generate massive amounts of data, causing a bottleneck for traditional analysis workflows. To enable eddy studies, we have developed an in situ workflow for the quantitative and qualitative analysis of MPAS-Ocean, a high-resolution ocean climate model, in collaboration with the ocean model research and development process. Planned eddy analysis at high spatial and temporal resolutions will not be possible with a postprocessing workflow due to various constraints, such as storage size and I/O time, but the in situ workflow enables it and scales well to ten-thousand processing elements.",
        "journal": "TVCG",
        "year": 2016,
        "citationCount": 21,
        "authors": "Jonathan Woodring, Mark Petersen, Andre Schmeißer, John Patchett, James P. Ahrens, Hans Hagen"
    }, {
        "title": "In situ depth maps based feature extraction and tracking",
        "abstract": "Parallel numerical simulation is a powerful tool used by scientists to study complex problems. It has been a common practice to save the simulation output to disk and then conduct post-hoc in-depth analyses of the saved data. System I/O capabilities have not kept pace as simulations have scaled up over time, so a common approach has been to output only subsets of the data to reduce I/O. However, as we are entering the era of peta- and exa-scale computing, this sub-sampling approach is no longer acceptable because too much valuable information is lost. In situ visualization has been shown a promising approach to the data problem at extreme-scale. We present a novel in situ solution using depth maps to enable post-hoc image-based visualization and feature extraction and tracking. An interactive interface is provided to allow for fine-tuning the generation of depth maps during the course of a simulation run to better capture the features of interest. We use several applications including one actual simulation run on a Cray XE6 supercomputer to demonstrate the effectiveness of our approach.",
        "journal": "LDAV",
        "year": 2015,
        "citationCount": 4,
        "authors": "Yucong Chris Ye, Yang Wang, Robert Miller, Kwan-Liu Ma, Kenji Ono"
    }, {
        "title": "A study of i/o methods for parallel visualization of large-scale data",
        "abstract": "This paper presents two parallel I/O methods for the visualization of time-varying volume data in a high-performance computing environment. We discuss the interplay between the parallel renderer, I/O strategy, and file system, and show the results of our study on the performance of the I/O strategies with and without MPI parallel I/O support. The targeted application is earthquake modeling using a large 3D unstructured mesh consisting of one hundred millions cells. Our test results on the HP/Compaq AlphaServer operated at the Pittsburgh Supercomputing Center demonstrate that the I/O methods effectively remove the I/O bottlenecks commonly present in time-varying data visualization, and therefore help significantly lower interframe delay. This high-performance visualization solution allows scientists to explore their data in the temporal, spatial, and visualization domains at high resolution. Such new explorability, likely not presently available to most computational science groups, will help lead to many new insights into the modeled physical and chemical processes.",
        "journal": "Parallel Computing",
        "year": 2005,
        "citationCount": 40,
        "authors": "Hongfeng Yu, Kwan-Liu Ma"
    }, {
        "title": "In situ visualization for large-scale combustion simulations",
        "abstract": "As scientific supercomputing moves toward petascale and exascale levels, in situ visualization stands out as a scalable way for scientists to view the data their simulations generate. This full picture is crucial particularly for capturing and understanding highly intermittent transient phenomena, such as ignition and extinction events in turbulent combustion.",
        "journal": "CG&A",
        "year": 2010,
        "citationCount": 175,
        "authors": "Hongfeng Yu, Chaoli Wang, Ray W. Grout, Jacqueline Chen, Kwan-Liu Ma"
    }, {
        "title": "Predata - preparatory data analytics on peta-scale machines",
        "abstract": "Peta-scale scientific applications running on High End Computing (HEC) platforms can generate large volumes of data. For high performance storage and in order to be useful to science end users, such data must be organized in its layout, indexed, sorted, and otherwise manipulated for subsequent data presentation, visualization, and detailed analysis. In addition, scientists desire to gain insights into selected data characteristics `hidden' or `latent' in these massive datasets while data is being produced by simulations. PreDatA, short for Preparatory Data Analytics, is an approach to preparing and characterizing data while it is being produced by the large scale simulations running on peta-scale machines. By dedicating additional compute nodes on the machine as `staging' nodes and by staging simulations' output data through these nodes, PreDatA can exploit their computational power to perform select data manipulations with lower latency than attainable by first moving data into file systems and storage. Such intransit manipulations are supported by the PreDatA middleware through asynchronous data movement to reduce write latency, application-specific operations on streaming data that are able to discover latent data characteristics, and appropriate data reorganization and metadata annotation to speed up subsequent data access. PreDatA enhances the scalability and flexibility of the current I/O stack on HEC platforms and is useful for data pre-processing, runtime data analysis and inspection, as well as for data exchange between concurrently running simulations.",
        "journal": "IPDPS",
        "year": 2010,
        "citationCount": 162,
        "authors": "Fang Zheng, Hasan Abbasi, Ciprian Docan, Jay Lofstead, Qing Liu, Scott Klasky, Manish Parashar, Norbert Podhorszki, Karsten Schwan, Matthew Wolf"
    }, {
        "title": "Canopus: A Paradigm Shift Towards Elastic Extreme-Scale Data Analytics on HPC Storage",
        "abstract": "Scientific simulations on high performance computing (HPC) platforms generate large quantities of data. To bridge the widening gap between compute and I/O, and enable data to be more efficiently stored and analyzed, simulation outputs need to be refactored, reduced, and appropriately mapped to storage tiers. However, a systematic solution to support these steps has been lacking on the current HPC software ecosystem. To that end, this paper develops Canopus, a progressive JPEGlike data management scheme for storing and analyzing big scientific data. It co-designs the data decimation, compression and data storage, taking the hardware characteristics of each storage tier into considerations. With reasonably low overhead, our approach refactors simulation data into a much smaller, reduced-accuracy base dataset, and a series of deltas that is used to augment the accuracy if needed. The base dataset and deltas are compressed and written to multiple storage tiers. Data saved on different tiers can then be selectively retrieved to restore the level of accuracy that satisfies data analytics. Thus, Canopus provides a paradigm shift towards elastic data analytics and enables end users to make trade-offs between analysis speed and accuracy on-the-fly. We evaluate the impact of Canopus on unstructured triangular meshes, a pervasive data model used by scientific modeling and simulations. In particular, we demonstrate the progressive data exploration of Canopus using the “blob detection” use case on the fusion simulation data.",
        "journal": "CLUSTER",
        "year": 2017,
        "citationCount": 1,
        "authors": "Tao Lu, Eric Suchyta, Dave Pugmire, Jong Choi, Scott Klasky, Qing Liu, Norbert Podhorszki, Mark Ainsworth, Matthew Wolf"
    }, {
        "title": "Massively Parallel Volume Rendering Using 2-3 Swap Image Compositing",
        "abstract": "The ever-increasing amounts of simulation data produced by scientists demand high-end parallel visualization capability. However, image compositing, which requires interprocessor communication, is often the bottleneck stage for parallel rendering of large volume data sets. Existing image compositing solutions either incur a large number of messages exchanged among processors (such as the direct send method), or limit the number of processors that can be effectively utilized (such as the binary swap method). We introduce a new image compositing algorithm, called 2-3 swap, which combines the flexibility of the direct send method and the optimality of the binary swap method. The 2-3 swap algorithm allows an arbitrary number of processors to be used for compositing, and fully utilizes all participating processors throughout the course of the compositing. We experiment with this image compositing solution on a supercomputer with thousands of processors, and demonstrate its great flexibility as well as scalability.",
        "journal": "SC",
        "year": 2008,
        "citationCount": 11,
        "authors": "Hongfeng Yu, Chaoli Wang, Kwan-Liu Ma"
    }, {
        "title": "Efficient Sort-Last Rendering Using Compression-Based Image Compositing",
        "abstract": "State of the art scientific simulations are currently working with data set sizes on the order of a billion cells. Parallel rendering is a promising approach for interactively visualizing multiple isosurface variables from data sets of this magnitude. In sort-last rendering, each processor creates a depth buffered image of its assigned objects. All processors’ images are composited together to create a final result. Improving the efficiency of this compositing step is key to interactive parallel rendering. This paper presents a compression-based image compositing algorithm which can provide significant savings in both communication and compositing costs.",
        "journal": "EGPGV",
        "year": 1998,
        "citationCount": 68,
        "authors": "James Ahrens, James Painter"
    }, {
        "title": "Explorable images for visualizing volume data",
        "abstract": "We present a technique which automatically converts a small number of single-view volume rendered images of the same 3D data set into a compact representation of that data set. This representation is a multi-layered image, or an explorable image, which enables interactive exploration of volume data in transfer function space without accessing the original data. We achieve this by automatically extracting layers depicted in composited images. The layers can then be recombined in different ways to simulate opacity changes and recoloring of individual features. Our results demonstrate that explorable images are especially useful when the volume data is too large for interactive exploration, takes too long to render due to the underlying mesh structure or desired shading effect, or if the original volume data is not available. Explorable images can offer real-time image-based interaction as a preview mechanism for remote visualization or visualization of large volume data on low-end hardware, within a mobile device, or a Web browser.",
        "journal": "PacificVis",
        "year": 2010,
        "citationCount": 10,
        "authors": "Anna Tikhonova, Carlos D. Correa, Kwan-Liu Ma"
    }
],
"relations": [
    {
        "citedBy": 24,
        "citing": 1
    }, {
        "citedBy": 24,
        "citing": 22
    }, {
        "citedBy": 24,
        "citing": 23
    }, {
        "citedBy": 24,
        "citing": 27
    }, {
        "citedBy": 2,
        "citing": 0
    }, {
        "citedBy": 2,
        "citing": 1
    }, {
        "citedBy": 2,
        "citing": 13
    }, {
        "citedBy": 19,
        "citing": 1
    }, {
        "citedBy": 19,
        "citing": 6
    }, {
        "citedBy": 19,
        "citing": 9
    }, {
        "citedBy": 19,
        "citing": 10
    }, {
        "citedBy": 19,
        "citing": 12
    }, {
        "citedBy": 19,
        "citing": 22
    }, {
        "citedBy": 3,
        "citing": 1
    }, {
        "citedBy": 3,
        "citing": 12
    }, {
        "citedBy": 3,
        "citing": 20
    }, {
        "citedBy": 25,
        "citing": 0
    }, {
        "citedBy": 25,
        "citing": 1
    }, {
        "citedBy": 25,
        "citing": 3
    }, {
        "citedBy": 25,
        "citing": 9
    }, {
        "citedBy": 25,
        "citing": 7
    }, {
        "citedBy": 25,
        "citing": 8
    }, {
        "citedBy": 25,
        "citing": 11
    }, {
        "citedBy": 25,
        "citing": 17
    }, {
        "citedBy": 25,
        "citing": 21
    }, {
        "citedBy": 25,
        "citing": 23
    }, {
        "citedBy": 15,
        "citing": 20
    }, {
        "citedBy": 1,
        "citing": 27
    }, {
        "citedBy": 1,
        "citing": 8
    }, {
        "citedBy": 1,
        "citing": 21
    }, {
        "citedBy": 12,
        "citing": 22
    }, {
        "citedBy": 12,
        "citing": 27
    }, {
        "citedBy": 12,
        "citing": 21
    }, {
        "citedBy": 12,
        "citing": 23
    }, {
        "citedBy": 12,
        "citing": 4
    }, {
        "citedBy": 12,
        "citing": 28
    }, {
        "citedBy": 12,
        "citing": 6
    }, {
        "citedBy": 8,
        "citing": 23
    }, {
        "citedBy": 6,
        "citing": 23
    }, {
        "citedBy": 6,
        "citing": 26
    }, {
        "citedBy": 6,
        "citing": 28
    }, {
        "citedBy": 11,
        "citing": 14
    }, {
        "citedBy": 11,
        "citing": 13
    }, {
        "citedBy": 11,
        "citing": 17
    }, {
        "citedBy": 11,
        "citing": 28
    }, {
        "citedBy": 17,
        "citing": 4
    }, {
        "citedBy": 17,
        "citing": 13
    }, {
        "citedBy": 5,
        "citing": 4
    }, {
        "citedBy": 5,
        "citing": 13
    }, {
        "citedBy": 5,
        "citing": 21
    }, {
        "citedBy": 5,
        "citing": 23
    }, {
        "citedBy": 5,
        "citing": 27
    }, {
        "citedBy": 5,
        "citing": 28
    }, {
        "citedBy": 23,
        "citing": 13
    }, {
        "citedBy": 23,
        "citing": 27
    }, {
        "citedBy": 22,
        "citing": 13
    }, {
        "citedBy": 9,
        "citing": 28
    }, {
        "citedBy": 10,
        "citing": 28
    }, {
        "citedBy": 10,
        "citing": 13
    }, {
        "citedBy": 10,
        "citing": 4
    }, {
        "citedBy": 21,
        "citing": 4
    }, {
        "citedBy": 21,
        "citing": 14
    }, {
        "citedBy": 21,
        "citing": 27
    }, {
        "citedBy": 0,
        "citing": 13
    }, {
        "citedBy": 0,
        "citing": 14
    }, {
        "citedBy": 0,
        "citing": 23
    }, {
        "citedBy": 0,
        "citing": 27
    }, {
        "citedBy": 27,
        "citing": 14
    }, {
        "citedBy": 28,
        "citing": 13
    }, {
        "citedBy": 28,
        "citing": 18
    }, {
        "citedBy": 28,
        "citing": 26
    }, {
        "citedBy": 4,
        "citing": 14
    }, {
        "citedBy": 13,
        "citing": 18
    }, {
        "citedBy": 29,
        "citing": 2
    }, {
        "citedBy": 21,
        "citing": 30
    }, {
        "citedBy": 30,
        "citing": 31
    }, {
        "citedBy": 27,
        "citing": 31
    }, {
        "citedBy": 21,
        "citing": 32
    }, {
        "citedBy": 32,
        "citing": 31
    }
]
}